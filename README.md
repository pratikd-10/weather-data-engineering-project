# weather-data-engineering-project

ğŸŒ¦ï¸ Weather Data Engineering Pipeline

An end-to-end Data Engineering project that ingests real-time weather data using APIs, streams it via Apache Kafka, stores it in AWS S3 using a Medallion Architecture (Bronzeâ€“Silverâ€“Gold), and processes it using Apache Spark on Databricks Free Edition.

This project is designed to simulate a real-world production pipeline while working within free-tier limitations.

ğŸ“Œ Project Architecture
Weather API
   â†“
Kafka Producer (Python)
   â†“
Kafka Topic
   â†“
Kafka Consumer (Python)
   â†“
AWS S3 (Bronze Layer)
   â†“
Databricks (Spark Processing)
   â†“
Silver Layer (Cleaned Data)
   â†“
Gold Layer (Aggregated Data)
   â†“
SQL Analytics & Visualization



ğŸ§° Tech Stack
Category	                Technologies
Programming	              Python
Cloud Storage	            AWS S3
Streaming	                Apache Kafka
Compute	                  AWS EC2
Containerization	        Docker
Data Processing	          Apache Spark (Databricks Free Edition)
Analytics	                Databricks SQL
Architecture	            Medallion (Bronzeâ€“Silverâ€“Gold)



ğŸ“ Repository Structure
weather-data-engineering/
â”‚
â”œâ”€â”€ producer/
â”‚   â””â”€â”€ producer.py              # Fetches weather data & sends to Kafka
â”‚
â”œâ”€â”€ consumer/
â”‚   â””â”€â”€ consumer.py              # Consumes Kafka data & stores in S3
â”‚
â”œâ”€â”€ ingestion/
â”‚   â””â”€â”€ weather_api_ingestion.py # Direct API â†’ S3 ingestion (Stage 1)
â”‚
â”œâ”€â”€ databricks/
â”‚   â”œâ”€â”€ bronze_to_silver.py      # Spark transformations
â”‚   â”œâ”€â”€ silver_to_gold.py        # Aggregations
â”‚   â””â”€â”€ analysis.sql             # SQL queries for analytics
â”‚
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml       # Kafka setup
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_weather.jsonl     # Sample dataset (for Databricks Free)
â”‚
â””â”€â”€ README.md




ğŸ”¹ Stage 1: Data Ingestion (API â†’ S3)
ğŸ¯ Goal

Collect raw weather data from an external API and store it securely in AWS S3.

ğŸ”§ Steps

Create S3 Bucket
weather-data-project-<unique-id>


Create folders

bronze/   # raw JSON data
silver/   # cleaned data
gold/     # aggregated data


Create IAM User

Programmatic access
AmazonS3FullAccess
Python Ingestion Script
Uses requests to fetch weather data
Uses boto3 to upload JSON files into S3 (bronze)

ğŸ“Œ Why S3?
S3 acts as a data lake, providing durability, scalability, and low cost.





ğŸ”¹ Stage 2: Streaming Layer (Kafka)
ğŸ¯ Goal

Introduce real-time streaming to decouple ingestion and storage.

ğŸ”§ Steps

Launch EC2 Instance
Ubuntu t3.micro
Install Docker
Kafka runs inside Docker containers
Kafka Producer
Fetches weather data from API
Sends messages to Kafka topic
Kafka Consumer
Reads messages from Kafka topic
Stores each record as JSON in S3 bronze folder

ğŸ“Œ Why Kafka?

Buffers data
Prevents data loss
Decouples producers and consumers

âš ï¸ Note:
Due to Databricks Free Edition limitations, Kafka â†’ Spark ingestion is simulated. Data is manually uploaded for Spark processing.





ğŸ”¹ Stage 3: Data Processing (Spark + Databricks)
ğŸ¯ Goal

Transform raw data into analytics-ready datasets.

ğŸ”§ Steps
1. Bronze Layer

Raw JSON/JSONL data uploaded manually into Databricks

2. Silver Layer

Flatten nested JSON
Convert timestamps
Remove invalid or null records

3. Gold Layer

Aggregated metrics:
Average temperature per city per day

ğŸ“Œ Why Medallion Architecture?
Separates raw, cleaned, and business-ready data
Improves data quality and maintainability




ğŸ“Š Analytics & Visualization

SQL queries run on Gold tables

Databricks SQL Editor used
Dashboards created using Databricks UI

Examples:
Daily average temperature trends
City-wise comparisons




ğŸ” Observability (Conceptual)

Row counts and basic metrics tracked
Demonstrates understanding of data quality monitoring
MLflow concepts applied where possible (limited in Free Edition)


ğŸš€ Future Enhancements

Full S3 â†” Databricks integration
Automated scheduling using Airflow
Real-time Spark Structured Streaming
Alerts on data quality issues



ğŸ Conclusion

This project demonstrates a practical, industry-aligned Data Engineering workflow, showcasing:

API ingestion
Streaming pipelines
Cloud storage
Spark-based transformations
Analytics-ready outputs

Despite free-tier constraints, the pipeline mirrors real production systems and is suitable for Data Engineering interviews and portfolios.




